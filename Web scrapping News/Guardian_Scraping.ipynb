{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Guardian Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwwTd9c0yfy3"
      },
      "source": [
        "import requests, pymongo, json, time\n",
        "from datetime import datetime, date, timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LKk07Tjyfy_"
      },
      "source": [
        "MY_API_KEY = open(\"C:/Users/proth/Desktop/Thesis/data/THE_GUARDIAN.key.txt\").read().strip()\n",
        "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
        "PAGE_SIZE = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsTNEBTwyfzA"
      },
      "source": [
        "def call_api(url, payload):\n",
        "    # Get the requested url. Error handling for bad requests should be done in\n",
        "    # the calling function.\n",
        "    return requests.get(url, params=payload)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1-L9HTMyfzB"
      },
      "source": [
        "def get_response(r):\n",
        "    # Use json.loads to read the response text\n",
        "    raw = json.loads(r.text)\n",
        "\n",
        "    # Return the meta (hits, etc.) and docs (containing urls'n'stuff) back\n",
        "    return raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVPpq_7kyfzC"
      },
      "source": [
        "def get_soup(url):\n",
        "    # Header to be passed in to NYT when scraping article text.\n",
        "    agent = 'DataWrangling/1.1; '\n",
        "    agent += 'monsuralirana@gmail.com)'\n",
        "    headers = {'user_agent': agent}\n",
        "\n",
        "    # Wrap in a try-except to prevent a maxTry connection error from erroring\n",
        "    # out the program. Return None if there are any issues.\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    # Just in case there was a normal error returned. Pass back None.\n",
        "    if r.status_code != 200: return None\n",
        "\n",
        "    # Otherwise return a soupified object containing the url text encoded in\n",
        "    # utf-8. Will toss back errors on some pages without the encoding in place.\n",
        "    return BeautifulSoup(r.text.encode('utf-8'), features=\"lxml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQrTqvwmyfzD"
      },
      "source": [
        "def get_body_text(docs):\n",
        "    # Grab the url from each document, if it exists, then scrape each url for\n",
        "    # its body text. If we get any errors along the way, continue on to the\n",
        "    # next document / url to be scraped.\n",
        "    result = []\n",
        "    for d in docs:\n",
        "        # article_dict = {}\n",
        "        # Make a copy of the doc's dictionary\n",
        "        doc = d.copy()\n",
        "        # If there's no url (not sure why this happens sometimes) then ditch it\n",
        "        if not doc['web_url']:\n",
        "            continue\n",
        "\n",
        "        # Scrape the doc's url, return a soup object with the url's text.\n",
        "        soup = get_soup(doc['web_url'])\n",
        "        if not soup:\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        # Find all of the paragraphs with the correct class.\n",
        "        # This class tag is specific to NYT articles.\n",
        "        body = soup.find_all('div', class_=\"StoryBodyCompanionColumn\")\n",
        "        if not body:\n",
        "            continue\n",
        "\n",
        "        # Join the resulting body paragraphs' text (returned in a list).\n",
        "        doc['body'] = '\\n'.join([x.get_text() for x in body])\n",
        "\n",
        "        print(doc['web_url'])\n",
        "        result.append(doc)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs3NCYrPyfzE"
      },
      "source": [
        "def remove_previously_scraped(coll, docs):\n",
        "    # Check to see if the mongo collection already contains the docs returned\n",
        "    # from NYT. Return back a list of the ones that aren't in the collection to\n",
        "    # be scraped.\n",
        "    new_docs = []\n",
        "    for doc in docs:\n",
        "        # Check fo the document id in mongo. If it finds none, append to\n",
        "        # new_docs\n",
        "        # cursor = articles.find({'_id': doc['_id']}).limit(1)\n",
        "        if not articles.count_documents(filter={'_id': doc['_id']}) > 0:\n",
        "            new_docs.append(doc)\n",
        "\n",
        "    if new_docs == []:\n",
        "        return None\n",
        "\n",
        "    return new_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9-8y5wyyfzG"
      },
      "source": [
        "def get_end_date(dt):\n",
        "    # String-ify the datetime object to YYYMMDD, which the NYT likes.\n",
        "    yr = str(dt.year)\n",
        "    mon = '0' * (2 - len(str(dt.month))) + str(dt.month)\n",
        "    day = '0' * (2 - len(str(dt.day))) + str(dt.day)\n",
        "    return yr + mon + day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kQyyKo3yfzH"
      },
      "source": [
        "def scrape_articles(coll, psize=20):\n",
        "    # Request all of the newest articles matching the search term\n",
        "    start_date = date(2018, 1, 1)\n",
        "    end_date = date(2018,11, 1)\n",
        "    dayrange = range((end_date - start_date).days + 1)\n",
        "    for daycount in dayrange:\n",
        "        dt = start_date + timedelta(days=daycount)\n",
        "        print(dt)\n",
        "        dstr = dt.strftime('%Y-%m-%d')\n",
        "        payload = {\n",
        "            'from-date': dstr,\n",
        "            'to-date': dstr,\n",
        "            'order-by': \"newest\",\n",
        "            'show-fields': 'all',\n",
        "            'page-size': psize,\n",
        "            'api-key': MY_API_KEY\n",
        "        }\n",
        "\n",
        "        r = call_api(API_ENDPOINT, payload)\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            return \"Fail {}\".format(r.status_code)\n",
        "\n",
        "        # Get the meta data & documents from the request\n",
        "        docs = get_response(r)\n",
        "\n",
        "        for doc in docs['response']['results']:\n",
        "            try:\n",
        "                # Insert each doc into Mongo\n",
        "                #doc\n",
        "                coll.insert_one(doc)\n",
        "            except:\n",
        "                # If there's any error writing it in the db, just move along.\n",
        "                continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEbFjCQAyfzI",
        "outputId": "08e0a8a4-cfeb-4861-df0b-996e854344f6"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initiate Mongo client\n",
        "    client = pymongo.MongoClient()\n",
        "\n",
        "\n",
        "    # Access database created for these articles\n",
        "    db = client.fake_news\n",
        "\n",
        "    articles = db.tg_articles\n",
        "\n",
        "    # Set the initial end date (scraper starts at this date and moves back in\n",
        "    # time sequentially)\n",
        "    last_date = datetime.now() + relativedelta(days=-2)\n",
        "    # print(last_date)\n",
        "\n",
        "    # Pass the database collection and initial end date into main function\n",
        "    scrape_articles(articles, PAGE_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2018-01-01\n",
            "2018-01-02\n",
            "2018-01-03\n",
            "2018-01-04\n",
            "2018-01-05\n",
            "2018-01-06\n",
            "2018-01-07\n",
            "2018-01-08\n",
            "2018-01-09\n",
            "2018-01-10\n",
            "2018-01-11\n",
            "2018-01-12\n",
            "2018-01-13\n",
            "2018-01-14\n",
            "2018-01-15\n",
            "2018-01-16\n",
            "2018-01-17\n",
            "2018-01-18\n",
            "2018-01-19\n",
            "2018-01-20\n",
            "2018-01-21\n",
            "2018-01-22\n",
            "2018-01-23\n",
            "2018-01-24\n",
            "2018-01-25\n",
            "2018-01-26\n",
            "2018-01-27\n",
            "2018-01-28\n",
            "2018-01-29\n",
            "2018-01-30\n",
            "2018-01-31\n",
            "2018-02-01\n",
            "2018-02-02\n",
            "2018-02-03\n",
            "2018-02-04\n",
            "2018-02-05\n",
            "2018-02-06\n",
            "2018-02-07\n",
            "2018-02-08\n",
            "2018-02-09\n",
            "2018-02-10\n",
            "2018-02-11\n",
            "2018-02-12\n",
            "2018-02-13\n",
            "2018-02-14\n",
            "2018-02-15\n",
            "2018-02-16\n",
            "2018-02-17\n",
            "2018-02-18\n",
            "2018-02-19\n",
            "2018-02-20\n",
            "2018-02-21\n",
            "2018-02-22\n",
            "2018-02-23\n",
            "2018-02-24\n",
            "2018-02-25\n",
            "2018-02-26\n",
            "2018-02-27\n",
            "2018-02-28\n",
            "2018-03-01\n",
            "2018-03-02\n",
            "2018-03-03\n",
            "2018-03-04\n",
            "2018-03-05\n",
            "2018-03-06\n",
            "2018-03-07\n",
            "2018-03-08\n",
            "2018-03-09\n",
            "2018-03-10\n",
            "2018-03-11\n",
            "2018-03-12\n",
            "2018-03-13\n",
            "2018-03-14\n",
            "2018-03-15\n",
            "2018-03-16\n",
            "2018-03-17\n",
            "2018-03-18\n",
            "2018-03-19\n",
            "2018-03-20\n",
            "2018-03-21\n",
            "2018-03-22\n",
            "2018-03-23\n",
            "2018-03-24\n",
            "2018-03-25\n",
            "2018-03-26\n",
            "2018-03-27\n",
            "2018-03-28\n",
            "2018-03-29\n",
            "2018-03-30\n",
            "2018-03-31\n",
            "2018-04-01\n",
            "2018-04-02\n",
            "2018-04-03\n",
            "2018-04-04\n",
            "2018-04-05\n",
            "2018-04-06\n",
            "2018-04-07\n",
            "2018-04-08\n",
            "2018-04-09\n",
            "2018-04-10\n",
            "2018-04-11\n",
            "2018-04-12\n",
            "2018-04-13\n",
            "2018-04-14\n",
            "2018-04-15\n",
            "2018-04-16\n",
            "2018-04-17\n",
            "2018-04-18\n",
            "2018-04-19\n",
            "2018-04-20\n",
            "2018-04-21\n",
            "2018-04-22\n",
            "2018-04-23\n",
            "2018-04-24\n",
            "2018-04-25\n",
            "2018-04-26\n",
            "2018-04-27\n",
            "2018-04-28\n",
            "2018-04-29\n",
            "2018-04-30\n",
            "2018-05-01\n",
            "2018-05-02\n",
            "2018-05-03\n",
            "2018-05-04\n",
            "2018-05-05\n",
            "2018-05-06\n",
            "2018-05-07\n",
            "2018-05-08\n",
            "2018-05-09\n",
            "2018-05-10\n",
            "2018-05-11\n",
            "2018-05-12\n",
            "2018-05-13\n",
            "2018-05-14\n",
            "2018-05-15\n",
            "2018-05-16\n",
            "2018-05-17\n",
            "2018-05-18\n",
            "2018-05-19\n",
            "2018-05-20\n",
            "2018-05-21\n",
            "2018-05-22\n",
            "2018-05-23\n",
            "2018-05-24\n",
            "2018-05-25\n",
            "2018-05-26\n",
            "2018-05-27\n",
            "2018-05-28\n",
            "2018-05-29\n",
            "2018-05-30\n",
            "2018-05-31\n",
            "2018-06-01\n",
            "2018-06-02\n",
            "2018-06-03\n",
            "2018-06-04\n",
            "2018-06-05\n",
            "2018-06-06\n",
            "2018-06-07\n",
            "2018-06-08\n",
            "2018-06-09\n",
            "2018-06-10\n",
            "2018-06-11\n",
            "2018-06-12\n",
            "2018-06-13\n",
            "2018-06-14\n",
            "2018-06-15\n",
            "2018-06-16\n",
            "2018-06-17\n",
            "2018-06-18\n",
            "2018-06-19\n",
            "2018-06-20\n",
            "2018-06-21\n",
            "2018-06-22\n",
            "2018-06-23\n",
            "2018-06-24\n",
            "2018-06-25\n",
            "2018-06-26\n",
            "2018-06-27\n",
            "2018-06-28\n",
            "2018-06-29\n",
            "2018-06-30\n",
            "2018-07-01\n",
            "2018-07-02\n",
            "2018-07-03\n",
            "2018-07-04\n",
            "2018-07-05\n",
            "2018-07-06\n",
            "2018-07-07\n",
            "2018-07-08\n",
            "2018-07-09\n",
            "2018-07-10\n",
            "2018-07-11\n",
            "2018-07-12\n",
            "2018-07-13\n",
            "2018-07-14\n",
            "2018-07-15\n",
            "2018-07-16\n",
            "2018-07-17\n",
            "2018-07-18\n",
            "2018-07-19\n",
            "2018-07-20\n",
            "2018-07-21\n",
            "2018-07-22\n",
            "2018-07-23\n",
            "2018-07-24\n",
            "2018-07-25\n",
            "2018-07-26\n",
            "2018-07-27\n",
            "2018-07-28\n",
            "2018-07-29\n",
            "2018-07-30\n",
            "2018-07-31\n",
            "2018-08-01\n",
            "2018-08-02\n",
            "2018-08-03\n",
            "2018-08-04\n",
            "2018-08-05\n",
            "2018-08-06\n",
            "2018-08-07\n",
            "2018-08-08\n",
            "2018-08-09\n",
            "2018-08-10\n",
            "2018-08-11\n",
            "2018-08-12\n",
            "2018-08-13\n",
            "2018-08-14\n",
            "2018-08-15\n",
            "2018-08-16\n",
            "2018-08-17\n",
            "2018-08-18\n",
            "2018-08-19\n",
            "2018-08-20\n",
            "2018-08-21\n",
            "2018-08-22\n",
            "2018-08-23\n",
            "2018-08-24\n",
            "2018-08-25\n",
            "2018-08-26\n",
            "2018-08-27\n",
            "2018-08-28\n",
            "2018-08-29\n",
            "2018-08-30\n",
            "2018-08-31\n",
            "2018-09-01\n",
            "2018-09-02\n",
            "2018-09-03\n",
            "2018-09-04\n",
            "2018-09-05\n",
            "2018-09-06\n",
            "2018-09-07\n",
            "2018-09-08\n",
            "2018-09-09\n",
            "2018-09-10\n",
            "2018-09-11\n",
            "2018-09-12\n",
            "2018-09-13\n",
            "2018-09-14\n",
            "2018-09-15\n",
            "2018-09-16\n",
            "2018-09-17\n",
            "2018-09-18\n",
            "2018-09-19\n",
            "2018-09-20\n",
            "2018-09-21\n",
            "2018-09-22\n",
            "2018-09-23\n",
            "2018-09-24\n",
            "2018-09-25\n",
            "2018-09-26\n",
            "2018-09-27\n",
            "2018-09-28\n",
            "2018-09-29\n",
            "2018-09-30\n",
            "2018-10-01\n",
            "2018-10-02\n",
            "2018-10-03\n",
            "2018-10-04\n",
            "2018-10-05\n",
            "2018-10-06\n",
            "2018-10-07\n",
            "2018-10-08\n",
            "2018-10-09\n",
            "2018-10-10\n",
            "2018-10-11\n",
            "2018-10-12\n",
            "2018-10-13\n",
            "2018-10-14\n",
            "2018-10-15\n",
            "2018-10-16\n",
            "2018-10-17\n",
            "2018-10-18\n",
            "2018-10-19\n",
            "2018-10-20\n",
            "2018-10-21\n",
            "2018-10-22\n",
            "2018-10-23\n",
            "2018-10-24\n",
            "2018-10-25\n",
            "2018-10-26\n",
            "2018-10-27\n",
            "2018-10-28\n",
            "2018-10-29\n",
            "2018-10-30\n",
            "2018-10-31\n",
            "2018-11-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ejELt-hyfzJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}